{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNetV1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "U74NGbY4508Z",
        "08Eytj_3oYDI",
        "2dnGV0FHocqH",
        "Dvhr1SS2ojnf",
        "apSpY04_op7G"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74NGbY4508Z",
        "colab_type": "text"
      },
      "source": [
        "# Import Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzmyPhhhvIRu",
        "colab_type": "code",
        "outputId": "7cace1eb-5944-4e27-f08b-4a6f50e8d4e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0b1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0b1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.14.0a20190603)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.16.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.0.8)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.11.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0b1) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Eytj_3oYDI",
        "colab_type": "text"
      },
      "source": [
        "# SPIT Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIMdpjQ-f-nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from importlib import reload\n",
        "from pkg_resources import resource_filename\n",
        "from google.colab import drive\n",
        "import imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkgCyHlig-fl",
        "colab_type": "code",
        "outputId": "3a501b2e-f5ea-4028-c5fd-b1a09f14feae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-beta1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqfsN-WdhAcm",
        "colab_type": "code",
        "outputId": "2c0f2550-a886-4dfd-a93b-f2b7f47e0369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znIJK2HDhBsO",
        "colab_type": "code",
        "outputId": "40b47ba8-97ef-41b1-ec6b-4d447f238981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/pypeit/spit.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'spit' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1kzH-dghJlh",
        "colab_type": "code",
        "outputId": "d171d3a9-f43c-42a2-8144-d9627b544ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd spit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spit/spit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCXppiJ_hLtu",
        "colab_type": "code",
        "outputId": "5da3b8a0-950f-4aea-e085-d5bfc8afdb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!git checkout x_fussing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already on 'x_fussing'\n",
            "Your branch is up to date with 'origin/x_fussing'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFBV0bnthNV8",
        "colab_type": "code",
        "outputId": "add311d4-bec0-46de-9784-0f0cad9a3a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI3uoAU0hOwc",
        "colab_type": "code",
        "outputId": "4dc311ca-5249-4ebe-d3d0-77becd2fc3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python setup.py develop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'setup.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1o27AbvhP7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "from importlib import reload\n",
        "\n",
        "from spit import image_loader\n",
        "from spit import labels\n",
        "from spit import preprocess\n",
        "from spit import classifier\n",
        "from spit import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dnGV0FHocqH",
        "colab_type": "text"
      },
      "source": [
        "# Build DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UK-KubhvJ4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veTuJQk36S0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_block(model):\n",
        "  \n",
        "  # 1x1 convolution bottleneck\n",
        "  \n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  model.add(keras.layers.ReLU())\n",
        "  #### HYPERPARAMETER ####\n",
        "  model.add(keras.layers.Conv2D(4*filters, kernel_size=1))\n",
        "  model.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "  \n",
        "  # 3x3 dense convolution\n",
        "  \n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  model.add(keras.layers.ReLU())\n",
        "  #### HYPERPARAMETER ####\n",
        "  model.add(keras.layers.Conv2D(filters, kernel_size=3))\n",
        "  #### HYPERPARAMETER ####\n",
        "  model.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "  \n",
        "  return\n",
        "\n",
        "def transition_layer(model):\n",
        "#   x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "#   x = Relu(x)\n",
        "  model.add(keras.layers.Conv2D(filters, kernel_size=1))\n",
        "  model.add(keras.layers.AveragePooling2D(pool_size=(2,2), strides=2))\n",
        "#   x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+'_conv1')\n",
        "#   x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
        "#   x = Average_pooling(x, pool_size=[2,2], stride=2)\n",
        "  return\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HozlILKAvVCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### DENSE NET ARCHITECTURE #####\n",
        "#\n",
        "# CONV --> BLOCK --> TRANSITION --> BLOCK --> TRANSITION --> BLOCK --> POOL --> SOFTMAX\n",
        "#\n",
        "# BLOCK:\n",
        "# BatchNorm --> ReLu --> Conv (1x1) --> DropOut --> BatchNorm --> ReLu --> Conv (3x3) --> DropOut --> output\n",
        "#\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# hyperparameters\n",
        "filters = 64\n",
        "dropout_rate = .5\n",
        "\n",
        "# add a conv layer\n",
        "model.add(keras.layers.Conv2D(filters, kernel_size=7, strides=(2, 2), padding='valid', activation='relu', \n",
        "                            input_shape = (210, 650, 1)))\n",
        "# add block\n",
        "dense_block(model)\n",
        "# add transition\n",
        "transition_layer(model)\n",
        "# add block\n",
        "dense_block(model)\n",
        "# add transition\n",
        "transition_layer(model)\n",
        "# add block\n",
        "dense_block(model)\n",
        "# add pool\n",
        "model.add(keras.layers.GlobalAveragePooling2D())\n",
        "# add flatten\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# Produce 0-1 probabilities with softmax\n",
        "model.add(keras.layers.Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwc1bmdCfmDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvhr1SS2ojnf",
        "colab_type": "text"
      },
      "source": [
        "# Fake Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UOxTk6beV3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.random.random((100, 210, 650, 1))\n",
        "train_labels = np.random.randint(5, size=(100, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YbIjCS-fhEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(train_labels, num_classes=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zquoTIHjfild",
        "colab_type": "code",
        "outputId": "dfc5cff3-0d06-4cd2-e41d-84eafb485e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        " model.fit(\n",
        "        x_train, \n",
        "        y_train, \n",
        "        epochs=5, \n",
        "        batch_size=6 \n",
        " )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100 samples\n",
            "Epoch 1/5\n",
            " 12/100 [==>...........................] - ETA: 2:54 - loss: 2.8118 - accuracy: 0.2500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e2f5f28a2416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apSpY04_op7G",
        "colab_type": "text"
      },
      "source": [
        "# Kast Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_qa1ZZphR7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  os.environ['SPIT_PATH'] = \"/content/drive/My Drive/Colab Notebooks/SPIT_DATA\"\n",
        "  os.environ['SAVE_PATH'] = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "  # might need to add subset\n",
        "  def _train(epochs, batch_size, subset_percent=None, train_images=None, train_labels=None, validation_data=None, steps_per_epoch=None, validation_freq=1, test_model=None, spit_path=os.getenv('SPIT_PATH'), save_path=os.getenv('SAVE_PATH')):\n",
        "    \"\"\"\n",
        "\n",
        "    Trains the classifier with given images, labels, and training parameters.\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "    :param epochs:\n",
        "      Number of epochs of the training. \n",
        "      Must be an integer value.\n",
        "\n",
        "    :param batch_size:\n",
        "      Size of the training batches formed in the process. \n",
        "      Must be an integer value.\n",
        "\n",
        "    :param save_path:\n",
        "      Path to where the best model will be saved.\n",
        "\n",
        "    :param train_images:\n",
        "       Set of images for model to train on.\n",
        "       Assume this is a numpy array with (batch_size, width, height, num_channels) as its dimensions.\n",
        "\n",
        "    :param train_labels:\n",
        "      Set of test labels corresponding to test images.\n",
        "      Assume this is a rank 1 array with (batch_size, ) as its dimensions.\n",
        "\n",
        "    :param validation_data:\n",
        "      Data to be used for the validation set. \n",
        "      Assume this is a tuple with (images, labels) with same dimensions as train_images, train_labels.\n",
        "      If None is specified, validation_data will be None.\n",
        "\n",
        "    :param steps_per_epoch:\n",
        "      Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch.\n",
        "      Assume this is an integer value. If None is specified, this will be None.  \n",
        "\n",
        "    :param validation_freq:\n",
        "      Specifies how many training epochs to run before a new validation run is performed.\n",
        "      Assume this is an integer value or a collection containing the epochs at which to run validation (ie [1,2,10]).\n",
        "\n",
        "    :param test_model:\n",
        "      An alternative choice for model to train on. Assume None.\n",
        "\n",
        "    :param spit_path & param save_path:\n",
        "      Path to the spit images and where the model will be saved respectively.\n",
        "      ***Environmental variables must be set by caller or the path must be passed manually.***\n",
        "\n",
        "    Returns:\n",
        "    :returns history:\n",
        "      Tensorflow History object containing loss and accuracy data over the training.\n",
        "\n",
        "    \"\"\"\n",
        "    # choose the model\n",
        "    if test_model is None:\n",
        "      model = test_model\n",
        "    else:\n",
        "      model = test_model\n",
        "\n",
        "    # if None is passed, then use the kast images\n",
        "    if train_images is None or train_labels is None:\n",
        "      # load training set\n",
        "      train = np.load(os.path.join(spit_path, 'Kast', 'kast_train.npz'))\n",
        "      train_images = train['images']\n",
        "      train_labels = train['labels']\n",
        "\n",
        "      # load validation set\n",
        "      validate = np.load(os.path.join(spit_path, 'Kast', 'kast_validate.npz'))\n",
        "      v_images = validate['images']\n",
        "      v_labels = validate['labels']\n",
        "\n",
        "      validation_data = (v_images, v_labels)\n",
        "\n",
        "    # change to categorical and make subsets\n",
        "    if validation_data is not None:\n",
        "      valid_images, valid_labels = validation_data\n",
        "      if subset_percent is not None:\n",
        "        valid_images, valid_labels = split_array(valid_images, valid_labels, subset_percent)\n",
        "      valid_labels = keras.utils.to_categorical(valid_labels, num_classes=len(label_dict))\n",
        "      validation_data = (valid_images, valid_labels)\n",
        "\n",
        "    if train_images is not None and train_labels is not None:\n",
        "      if subset_percent is not None:\n",
        "        train_images, train_labels = split_array(train_images, train_labels, subset_percent)\n",
        "      train_labels = keras.utils.to_categorical(train_labels, num_classes=len(label_dict))\n",
        "\n",
        "    # checkpoint to track best model\n",
        "    checkpoint=keras.callbacks.ModelCheckpoint(save_path+'best_model.h5', monitor='val_acc', save_best_only=True, mode='max')\n",
        "\n",
        "    # train the model\n",
        "    history = model.fit(\n",
        "          train_images, \n",
        "          train_labels, \n",
        "          epochs=epochs, \n",
        "          batch_size=batch_size,\n",
        "          validation_data=validation_data,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          validation_freq=validation_freq,\n",
        "          callbacks=[checkpoint]\n",
        "    )\n",
        "    # loss and accuracy data\n",
        "    keys = history.history\n",
        "    # save to disc differently based on whether validation set was used\n",
        "#     if len(history.history.keys()) == 2: # can we abstract this away?\n",
        "#       np.savez_compressed('history.npz', loss=keys['loss'], acc=keys['acc']) #can these keys be abstracted away?\n",
        "#     else:\n",
        "#       np.savez_compressed('history.npz', loss=keys['loss'], acc=keys['acc'], val_loss=keys['val_loss'], val_acc=keys['val_acc'])\n",
        "\n",
        "    return history\n",
        "\n",
        "  import math\n",
        "\n",
        "  def split_array(images, labels, subset_percent):\n",
        "    \"\"\"\n",
        "    Splits dataset based on a percentage value.\n",
        "\n",
        "    Parameters:\n",
        "    :param images:\n",
        "      Images from a dataset to be trained on.\n",
        "      4-D Numpy array with (batch_size, width, height, num_channels) as its dimensions.\n",
        "\n",
        "    :param labels:\n",
        "      Labels from a dataset to be trained on.\n",
        "      Rank 1 Numpy array with (batch_size,) as its dimensions.\n",
        "\n",
        "    :param subset_percent:\n",
        "      Float value determining percentage of subset to remain.\n",
        "\n",
        "    Returns:\n",
        "    :returns split_images:\n",
        "      Numpy array containing a fraction of the initial images parameter (batch_size*subset_percent)\n",
        "\n",
        "    :returns split_labels:\n",
        "      Numpy array containing a fraction of the initial labels parameter (batch_size*subset_percent)\n",
        "    \"\"\"\n",
        "    split_images = []\n",
        "    split_labels = []\n",
        "    \n",
        "    # get all unique labels\n",
        "    uni_lbls = np.unique(labels)\n",
        "    \n",
        "    # find all instances of labels and subset based on that\n",
        "    for uni_lbl in uni_lbls:\n",
        "      idx = np.where(labels==uni_lbl)[0]\n",
        "      # 0 : len(idx)*subset_percent\n",
        "      lower = 0\n",
        "      upper = int(math.floor(len(idx)*subset_percent))\n",
        "      split_images.extend(images[idx[lower:upper]])\n",
        "      split_labels.extend(labels[idx[lower:upper]])\n",
        "    return np.asarray(split_images), np.asarray(split_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7R3gYehrjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = labels.kast_label_dict()\n",
        "preproc_dict = preprocess.original_preproc_dict()\n",
        "classify_dict = labels.kast_classify_dict(label_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hea_JEG7hsRn",
        "colab_type": "code",
        "outputId": "7bda9721-5735-432b-a6de-19de541a78d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "history = _train(epochs=5, batch_size=6, subset_percent = .05, test_model=model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 775 samples, validate on 190 samples\n",
            "Epoch 1/5\n",
            "774/775 [============================>.] - ETA: 1s - loss: 1.1711 - accuracy: 0.5323 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 20:52:36.457918 140551106664320 callbacks.py:986] Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r775/775 [==============================] - 1498s 2s/sample - loss: 1.1718 - accuracy: 0.5316 - val_loss: 12.1867 - val_accuracy: 0.2000\n",
            "Epoch 2/5\n",
            "774/775 [============================>.] - ETA: 1s - loss: 0.7773 - accuracy: 0.7003 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 21:17:27.445801 140551106664320 callbacks.py:986] Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r775/775 [==============================] - 1491s 2s/sample - loss: 0.7765 - accuracy: 0.7006 - val_loss: 0.8673 - val_accuracy: 0.6842\n",
            "Epoch 3/5\n",
            "774/775 [============================>.] - ETA: 1s - loss: 0.7296 - accuracy: 0.7674 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 21:42:17.369124 140551106664320 callbacks.py:986] Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r775/775 [==============================] - 1490s 2s/sample - loss: 0.7322 - accuracy: 0.7665 - val_loss: 1.6048 - val_accuracy: 0.5263\n",
            "Epoch 4/5\n",
            "774/775 [============================>.] - ETA: 1s - loss: 0.6023 - accuracy: 0.7804 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 22:07:09.766519 140551106664320 callbacks.py:986] Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r775/775 [==============================] - 1492s 2s/sample - loss: 0.6113 - accuracy: 0.7794 - val_loss: 1.0964 - val_accuracy: 0.6947\n",
            "Epoch 5/5\n",
            "774/775 [============================>.] - ETA: 1s - loss: 0.5828 - accuracy: 0.7855 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 22:31:57.023474 140551106664320 callbacks.py:986] Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r775/775 [==============================] - 1487s 2s/sample - loss: 0.5828 - accuracy: 0.7858 - val_loss: 3.1274 - val_accuracy: 0.5368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-12f7ff70d56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-5c42cd8a7b79>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(epochs, batch_size, subset_percent, train_images, train_labels, validation_data, steps_per_epoch, validation_freq, test_model, spit_path, save_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'history.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#can these keys be abstracted away?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'history.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lrQn4dJiIJJ",
        "colab_type": "code",
        "outputId": "98119973-d0c6-48a1-bbb0-5693f3793081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "utils.display_training_trends(history, key1='acc', key2='val_acc')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-a399507f08ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_training_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2gL32rpHo_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}